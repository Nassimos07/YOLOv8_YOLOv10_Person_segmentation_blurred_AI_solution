{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $BlurVision: \\space AI-Based \\space ROI \\space Protection  \\space for \\space  Copyright \\space  Compliance$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Setup$ ‚úÖ\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "# Check for CUDA device and set it\n",
    "torch.cuda.set_device(0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "# checks\n",
    "from ultralytics import YOLO, checks, hub\n",
    "checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from supervision.draw.color import ColorPalette\n",
    "from supervision import Detections, BoxAnnotator\n",
    "smoother = sv.DetectionsSmoother()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervision import ColorLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Model$ $Selection$ üçá\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8x-seg.pt\").to(device)  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Region$ $of $ $Interest$ $Selection$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_PATH='Videos\\7293938-hd_1920_1080_30fps.mp4'\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
    "video_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Utils$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_center(box):\n",
    "    \"\"\"Calculate the center point coordinates of a bounding box.\n",
    "    Args:\n",
    "        box (numpy.ndarray): A numpy array containing the bounding box coordinates in the form (xmin, ymin, xmax, ymax).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the coordinates of the center point in the form (center_x, center_y).\n",
    "    \"\"\"\n",
    "    box=box.astype(int)\n",
    "    center_x = (box[0] + box[2]) / 2\n",
    "    center_y = (box[1] + box[3]) / 2\n",
    "    return np.array([center_x, center_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_point_inside_bounding_box(point, bounding_box):\n",
    "    # Unpack the point and bounding box coordinates\n",
    "    x, y = point\n",
    "    x_min, y_min, x_max, y_max = bounding_box\n",
    "\n",
    "    # Check if the point is inside the bounding box\n",
    "    if x_min <= x <= x_max and y_min <= y <= y_max:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse callback function\n",
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global ix, iy, drawing, frame, rectangle_defined, rectangle_coords\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix, iy = x, y\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            frame_copy = frame.copy()\n",
    "            cv2.rectangle(frame_copy, (ix, iy), (x, y), (0, 255, 0), 4)\n",
    "            cv2.imshow('Video', frame_copy)\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        rectangle_defined = True\n",
    "        rectangle_coords = (ix, iy, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $RoI \\space Selection$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rectangle coordinates: [ 254  178  874 2362]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- Region of Interest Selction by User ---------------------------\n",
    "tracker = sv.ByteTrack(frame_rate=video_info.fps)\n",
    "\n",
    "# Initialize variables\n",
    "drawing = False                 # True if the mouse is pressed\n",
    "ix, iy = -1, -1                 # Initial position of the rectangle\n",
    "rectangle_defined = False       # To check if a rectangle has been defined\n",
    "rectangle_coords = None         # To store the coordinates of the defined rectangle\n",
    "\n",
    "\n",
    "cap2 = cv2.VideoCapture(SOURCE_VIDEO_PATH)  # Replace with 1 for the second camera\n",
    "\n",
    "\n",
    "if not cap2.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "cv2.namedWindow('Video')\n",
    "cv2.setMouseCallback('Video', draw_rectangle)\n",
    "\n",
    "\n",
    "original_width=video_info.width\n",
    "original_height=video_info.width\n",
    "\n",
    "scale_x = original_width / 920\n",
    "scale_y = original_height / 560\n",
    "\n",
    "\n",
    "# Read until video is completed\n",
    "while cap2.isOpened():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap2.read()\n",
    "    if ret:\n",
    "        \n",
    "        # Resize the frame\n",
    "        \n",
    "        result=model.track(frame,classes=[0])[0]\n",
    "        detections = sv.Detections.from_ultralytics(result)\n",
    "        detections = tracker.update_with_detections(detections)\n",
    "        \n",
    "        # bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=ColorLookup.TRACK)\n",
    "        # frame = bounding_box_annotator.annotate(\n",
    "        #     scene=frame.copy(),\n",
    "        #     detections=detections\n",
    "        # )\n",
    "        # Display the frame\n",
    "        frame = cv2.resize(frame, (1300, 700))\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(100000) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "# When everything done, release the video capture object\n",
    "cap2.release()\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()\n",
    "rectangle_coords=list(rectangle_coords)\n",
    "rectangle_coords[0] *= scale_x  # Scale x coordinates\n",
    "rectangle_coords[2] *= scale_x  # Scale x coordinates\n",
    "rectangle_coords[1] *= scale_y  # Scale y coordinates\n",
    "rectangle_coords[3] *= scale_y  # Scale y coordinates\n",
    "rectangle_coords=np.array(rectangle_coords).astype(int)\n",
    "print(f\"Rectangle coordinates: {rectangle_coords}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Processing$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[452]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap2 = cv2.VideoCapture(SOURCE_VIDEO_PATH)  # Replace with 1 for the second camera\n",
    "# Video Saver / Video Writer - results()\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "out = cv2.VideoWriter(f'blured.mp4', fourcc, video_info.fps, (video_info.width, video_info.height))\n",
    "\n",
    "if not cap2.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "cv2.namedWindow('Video')\n",
    "first_attempt = True\n",
    "\n",
    "while cap2.isOpened():\n",
    "    ret, frame = cap2.read()\n",
    "    if ret:\n",
    "        \n",
    "        result = model.track(frame, classes=[0],verbose=False)[0]\n",
    "        detections = sv.Detections.from_ultralytics(result)\n",
    "        detections = tracker.update_with_detections(detections)\n",
    "\n",
    "        if first_attempt:\n",
    "            key = [tracker_id\n",
    "                   for xyxy, _, _, _, tracker_id, _, in detections \n",
    "                   if is_point_inside_bounding_box(get_bbox_center(xyxy), rectangle_coords)]\n",
    "            print(key)\n",
    "            key = key[0]\n",
    "            first_attempt = False\n",
    "        \n",
    "        new_classes = np.array([1 if key == ID else 0 for ID in detections.tracker_id])\n",
    "        detections.class_id = new_classes\n",
    "\n",
    "        # Filtering detections to keep only the ROI\n",
    "        selected_classes = [1] \n",
    "        detections = detections[np.isin(detections.class_id, selected_classes)]\n",
    "\n",
    "        # Assuming detection mask exists for the ROI\n",
    "        if hasattr(detections, 'mask') :\n",
    "            if len(detections.mask)==0:\n",
    "                mask=np.zeros((1080,1920))\n",
    "            else:\n",
    "                # Remove the extra dimension from the mask\n",
    "                mask = np.squeeze(detections.mask.astype(np.uint8))  # Now (560, 920)\n",
    "\n",
    "            # Ensure the mask is single-channel and matches the frame size\n",
    "            if mask.shape != frame.shape[:2]:\n",
    "                mask = cv2.resize(mask, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "            # Step 1: Dilate the mask to expand it slightly\n",
    "            kernel = np.ones((15, 15), np.uint8)  # Adjust as needed\n",
    "            mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "            mask=mask.astype(float)\n",
    "\n",
    "\n",
    "            # Step 2: Create a gradient (feather) effect by applying a large Gaussian blur\n",
    "            feathered_mask = cv2.GaussianBlur(mask, (31,31), 0)\n",
    "\n",
    "            # Normalize the feathered mask to range between 0 and 1\n",
    "            # feathered_mask = feathered_mask.astype(float) / 255.0\n",
    "\n",
    "            # Apply blur to the whole image\n",
    "            blurred_frame = cv2.GaussianBlur(frame, (7, 7), 100,100)\n",
    "\n",
    "\n",
    "            # Step 3: Blend the original image and blurred image using the feathered mask\n",
    "            final_frame = (frame * feathered_mask[..., np.newaxis] + \n",
    "                           blurred_frame * (1 - feathered_mask[..., np.newaxis])).astype(np.uint8)\n",
    "        else:\n",
    "            final_frame = frame\n",
    "\n",
    "\n",
    "        out.write(final_frame)\n",
    "        final_frame = cv2.resize(final_frame, (920, 560))\n",
    "        cv2.imshow('Video', final_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap2.release()\n",
    "out.release()\n",
    "# Close all the frames\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
